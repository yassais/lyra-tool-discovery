{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"llm.energy","text":"<p>Extract documentation for AI agents from any site with llms.txt support.</p>"},{"location":"#what-is-llmenergy","title":"What is llm.energy?","text":"<p>llm.energy fetches documentation from websites that support the llms.txt standard and organizes it into downloadable markdown files optimized for AI assistants.</p> <ul> <li> <p> Extract</p> <p>Fetch <code>llms.txt</code> or <code>llms-full.txt</code> from any documentation site</p> </li> <li> <p> Organize</p> <p>Split content into individual markdown files by section</p> </li> <li> <p> Agent-Ready</p> <p>Includes <code>AGENT-GUIDE.md</code> with instructions for AI assistants</p> </li> <li> <p> Download</p> <p>Get individual files or everything as a ZIP archive</p> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Web App</p> <ol> <li>Visit llm.energy</li> <li>Enter a documentation URL</li> <li>Download the extracted files</li> </ol> <p>MCP Server</p> <pre><code>{\n  \"mcpServers\": {\n    \"llm-energy\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@llm-energy/mcp-server\"]\n    }\n  }\n}\n</code></pre>"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>MCP Server</li> <li>API Reference</li> </ul>"},{"location":"api-reference/","title":"API Reference","text":"<p>REST API for llm.energy.</p>"},{"location":"api-reference/#base-url","title":"Base URL","text":"<pre><code>https://llm.energy/api\n</code></pre>"},{"location":"api-reference/#endpoints","title":"Endpoints","text":""},{"location":"api-reference/#extract-documentation","title":"Extract Documentation","text":"<pre><code>POST /extract\n</code></pre> <p>Request Body</p> <pre><code>{\n  \"url\": \"docs.anthropic.com\"\n}\n</code></pre> <p>Response</p> <pre><code>{\n  \"success\": true,\n  \"source\": \"llms-full.txt\",\n  \"documents\": [\n    {\n      \"name\": \"getting-started.md\",\n      \"title\": \"Getting Started\",\n      \"tokens\": 1250\n    }\n  ],\n  \"stats\": {\n    \"totalDocuments\": 12,\n    \"totalTokens\": 45000,\n    \"processingTimeMs\": 234\n  }\n}\n</code></pre>"},{"location":"api-reference/#fetch-raw-content","title":"Fetch Raw Content","text":"<pre><code>GET /fetch?url={url}&amp;full={boolean}\n</code></pre> <p>Parameters</p> Name Type Default Description <code>url</code> string required Documentation site URL <code>full</code> boolean <code>true</code> Fetch llms-full.txt <p>Response</p> <pre><code>{\n  \"success\": true,\n  \"content\": \"# Documentation\\n\\n...\",\n  \"source\": \"llms-full.txt\"\n}\n</code></pre>"},{"location":"api-reference/#error-responses","title":"Error Responses","text":"<pre><code>{\n  \"success\": false,\n  \"error\": \"No llms.txt found at the specified URL\"\n}\n</code></pre> Status Description 400 Invalid request parameters 404 No llms.txt found 500 Server error"},{"location":"examples/","title":"Examples","text":"<p>Real-world examples of using llm.energy.</p>"},{"location":"examples/#extracting-anthropic-docs","title":"Extracting Anthropic Docs","text":"<pre><code># Web App\nhttps://llm.energy\n\n# Enter URL\ndocs.anthropic.com\n</code></pre> <p>Result: 15+ markdown files covering Claude API, prompt engineering, and more.</p>"},{"location":"examples/#extracting-mcp-docs","title":"Extracting MCP Docs","text":"<pre><code># URL\nmodelcontextprotocol.io\n</code></pre> <p>Result: Complete MCP specification, server implementation guides, and client examples.</p>"},{"location":"examples/#extracting-stripe-docs","title":"Extracting Stripe Docs","text":"<pre><code># URL\ndocs.stripe.com\n</code></pre> <p>Result: API reference, integration guides, and webhook documentation.</p>"},{"location":"examples/#using-with-claude","title":"Using with Claude","text":"<ol> <li>Extract documentation using llm.energy</li> <li>Download the ZIP archive</li> <li>In Claude, start a new conversation</li> <li>Upload <code>llms-full.md</code> or individual files</li> <li>Ask questions about the documentation</li> </ol> <p>Example prompt:</p> <p>\"Based on the uploaded documentation, how do I implement streaming responses?\"</p>"},{"location":"examples/#using-with-chatgpt","title":"Using with ChatGPT","text":"<ol> <li>Extract documentation</li> <li>Download files</li> <li>Upload to ChatGPT (GPT-4 with file upload)</li> <li>Reference the docs in your prompts</li> </ol>"},{"location":"examples/#using-the-mcp-server","title":"Using the MCP Server","text":"<p>With the MCP server, AI assistants can extract docs programmatically:</p> <p>Claude Desktop</p> <pre><code>You: \"Extract the Vercel documentation and tell me about edge functions\"\n\nClaude: [Uses extract_documentation tool]\n        [Reads relevant sections]\n        \"Based on the Vercel docs, edge functions are...\"\n</code></pre> <p>Example conversation:</p> <pre><code>You: \"What MCP tools do you have for documentation?\"\n\nClaude: \"I have access to llm-energy with these tools:\n        - extract_documentation: Extract docs from a URL\n        - fetch_llms_txt: Get raw llms.txt content\n        - get_document: Retrieve a cached document\n        - list_documents: List all cached docs\"\n\nYou: \"Extract docs from docs.anthropic.com\"\n\nClaude: [Extracts and summarizes the documentation]\n</code></pre>"},{"location":"examples/#supported-sites","title":"Supported Sites","text":"<p>Sites known to have llms.txt support:</p> Site URL Anthropic docs.anthropic.com MCP modelcontextprotocol.io Stripe docs.stripe.com Vercel vercel.com/docs Supabase supabase.com/docs Mintlify mintlify.com/docs <p>Check any site</p> <p>Try adding <code>/llms.txt</code> or <code>/llms-full.txt</code> to any documentation URL to see if it's supported.</p>"},{"location":"llms-txt-standard/","title":"The llms.txt Standard","text":"<p>Understanding the llms.txt standard for AI-readable documentation.</p>"},{"location":"llms-txt-standard/#what-is-llmstxt","title":"What is llms.txt?","text":"<p>The llms.txt standard is a convention for providing documentation in a format optimized for Large Language Models (LLMs).</p> <p>Similar to how <code>robots.txt</code> tells search engines how to crawl a site, <code>llms.txt</code> provides AI assistants with structured documentation.</p>"},{"location":"llms-txt-standard/#file-locations","title":"File Locations","text":"File Purpose <code>/llms.txt</code> Concise overview and key information <code>/llms-full.txt</code> Complete documentation"},{"location":"llms-txt-standard/#format","title":"Format","text":"<p>llms.txt files are plain markdown:</p> <pre><code># Project Name\n\n&gt; Brief description\n\n## Section 1\n\nContent...\n\n## Section 2\n\nContent...\n</code></pre>"},{"location":"llms-txt-standard/#why-it-matters","title":"Why It Matters","text":"<p>For AI Assistants:</p> <ul> <li>Structured content that's easy to parse</li> <li>No HTML/CSS/JS to filter out</li> <li>Complete documentation in one request</li> </ul> <p>For Developers:</p> <ul> <li>AI assistants give better answers about your product</li> <li>Reduces hallucination by providing accurate source material</li> <li>One source of truth for AI-consumed docs</li> </ul>"},{"location":"llms-txt-standard/#adoption","title":"Adoption","text":"<p>Sites with llms.txt support include:</p> <ul> <li>Anthropic</li> <li>Model Context Protocol</li> <li>Stripe</li> <li>Vercel</li> <li>Mintlify</li> </ul>"},{"location":"llms-txt-standard/#adding-llmstxt-to-your-site","title":"Adding llms.txt to Your Site","text":"<p>Option 1: Manual</p> <p>Create <code>/public/llms.txt</code> or <code>/public/llms-full.txt</code> with your documentation in markdown format.</p> <p>Option 2: Automated</p> <p>Documentation platforms like Mintlify automatically generate llms.txt from your docs.</p> <p>Best Practices:</p> <ul> <li>Include all essential documentation</li> <li>Use clear markdown headers</li> <li>Keep content up to date with your main docs</li> <li>Provide both concise (<code>llms.txt</code>) and full (<code>llms-full.txt</code>) versions</li> </ul>"},{"location":"llms-txt-standard/#learn-more","title":"Learn More","text":"<ul> <li>llmstxt.org - Official standard</li> <li>Anthropic's llms.txt - Example implementation</li> </ul>"},{"location":"overview/","title":"Overview","text":"<p>How llm.energy works.</p>"},{"location":"overview/#the-llmstxt-standard","title":"The llms.txt Standard","text":"<p>Many documentation sites provide their content in a machine-readable format at <code>/llms.txt</code> or <code>/llms-full.txt</code>. This format is designed for AI consumption.</p> Endpoint Description <code>/llms.txt</code> Concise version <code>/llms-full.txt</code> Complete documentation"},{"location":"overview/#how-it-works","title":"How It Works","text":"<pre><code>flowchart LR\n    A[Enter URL] --&gt; B[Fetch llms.txt]\n    B --&gt; C[Parse Content]\n    C --&gt; D[Generate Files]\n    D --&gt; E[Download]</code></pre>"},{"location":"overview/#1-fetch","title":"1. Fetch","text":"<p>llm.energy fetches:</p> <ol> <li><code>{url}/llms-full.txt</code> (complete documentation)</li> <li>Falls back to <code>{url}/llms.txt</code> if full version unavailable</li> </ol>"},{"location":"overview/#2-parse","title":"2. Parse","text":"<p>Content is split into sections based on markdown headers:</p> <ul> <li><code>##</code> headers become document boundaries</li> <li>Each section becomes its own <code>.md</code> file</li> <li>Filenames are generated from section titles</li> </ul>"},{"location":"overview/#3-generate","title":"3. Generate","text":"<p>Three types of output:</p> File Description <code>*.md</code> Individual section files <code>llms-full.md</code> Consolidated document with TOC <code>AGENT-GUIDE.md</code> Instructions for AI assistants"},{"location":"overview/#supported-sites","title":"Supported Sites","text":"<p>Any site that provides <code>/llms.txt</code> or <code>/llms-full.txt</code> will work:</p> <ul> <li>modelcontextprotocol.io</li> <li>docs.anthropic.com</li> <li>docs.stripe.com</li> <li>And many more...</li> </ul>"},{"location":"self-hosting/","title":"Self Hosting","text":"<p>Deploy llm.energy on your own infrastructure.</p>"},{"location":"self-hosting/#requirements","title":"Requirements","text":"<ul> <li>Node.js 18+</li> <li>pnpm (recommended)</li> </ul>"},{"location":"self-hosting/#clone-install","title":"Clone &amp; Install","text":"<pre><code>git clone https://github.com/nirholas/lyra-tool-discovery.git\ncd lyra-tool-discovery/llms-forge\npnpm install\n</code></pre>"},{"location":"self-hosting/#development","title":"Development","text":"<pre><code>pnpm dev\n</code></pre> <p>Open http://localhost:3001</p>"},{"location":"self-hosting/#production-build","title":"Production Build","text":"<pre><code>pnpm build\npnpm start\n</code></pre>"},{"location":"self-hosting/#deploy-to-vercel","title":"Deploy to Vercel","text":"<p>Or via CLI:</p> <pre><code>cd llms-forge\nvercel --prod\n</code></pre>"},{"location":"self-hosting/#environment-variables","title":"Environment Variables","text":"Variable Description Default <code>PORT</code> Server port <code>3001</code>"},{"location":"self-hosting/#mcp-server","title":"MCP Server","text":"<p>To run the MCP server locally:</p> <pre><code>cd llms-forge/mcp-server\npnpm install\npnpm build\npnpm start\n</code></pre>"},{"location":"mcp-server/installation/","title":"Installation","text":"<p>Set up the llm.energy MCP server for AI assistants.</p>"},{"location":"mcp-server/installation/#what-is-mcp","title":"What is MCP?","text":"<p>The Model Context Protocol (MCP) allows AI assistants to interact with external tools and data sources.</p>"},{"location":"mcp-server/installation/#quick-setup","title":"Quick Setup","text":"npx (Recommended)Global Install <p>No installation required:</p> <pre><code>{\n  \"mcpServers\": {\n    \"llm-energy\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@llm-energy/mcp-server\"]\n    }\n  }\n}\n</code></pre> <pre><code>npm install -g @llm-energy/mcp-server\n</code></pre> <pre><code>{\n  \"mcpServers\": {\n    \"llm-energy\": {\n      \"command\": \"llm-energy-mcp\"\n    }\n  }\n}\n</code></pre>"},{"location":"mcp-server/installation/#configuration","title":"Configuration","text":"Claude DesktopVS Code / Cursor <p>Add to your Claude Desktop config:</p> OS Path macOS <code>~/Library/Application Support/Claude/claude_desktop_config.json</code> Windows <code>%APPDATA%\\Claude\\claude_desktop_config.json</code> <p>Add to workspace settings or MCP configuration:</p> <pre><code>{\n  \"llm-energy\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@llm-energy/mcp-server\"]\n  }\n}\n</code></pre>"},{"location":"mcp-server/installation/#verify-installation","title":"Verify Installation","text":"<p>After configuration, restart your AI client. Ask:</p> <p>\"What tools does llm-energy provide?\"</p> <p>The assistant should list the available extraction tools.</p>"},{"location":"mcp-server/tools/","title":"Tools Reference","text":"<p>Available MCP tools for llm.energy.</p>"},{"location":"mcp-server/tools/#extract_documentation","title":"extract_documentation","text":"<p>Extract and parse documentation from a website with llms.txt support.</p> <p>Parameters</p> Name Type Required Description <code>url</code> string Yes Documentation site URL <p>Example</p> <pre><code>{\n  \"url\": \"docs.anthropic.com\"\n}\n</code></pre> <p>Output</p> <ul> <li>Extraction summary</li> <li>List of parsed documents with token counts</li> <li>Statistics (processing time, document count, total tokens)</li> </ul> <p>Caching</p> <p>After extraction, documentation is cached for quick access via other tools.</p>"},{"location":"mcp-server/tools/#fetch_llms_txt","title":"fetch_llms_txt","text":"<p>Fetch raw llms.txt content without parsing.</p> <p>Parameters</p> Name Type Required Description <code>url</code> string Yes Documentation site URL <code>full</code> boolean No Fetch llms-full.txt instead (default: true) <p>Example</p> <pre><code>{\n  \"url\": \"modelcontextprotocol.io\",\n  \"full\": true\n}\n</code></pre>"},{"location":"mcp-server/tools/#get_document","title":"get_document","text":"<p>Retrieve a specific document from the cache.</p> <p>Parameters</p> Name Type Required Description <code>name</code> string Yes Document filename <p>Example</p> <pre><code>{\n  \"name\": \"getting-started.md\"\n}\n</code></pre>"},{"location":"mcp-server/tools/#list_documents","title":"list_documents","text":"<p>List all cached documents.</p> <p>Output</p> <p>Array of document names with metadata:</p> <ul> <li>Filename</li> <li>Token count</li> <li>Section title</li> </ul>"}]}